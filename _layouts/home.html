<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Sangeek Hyun</title>

    <meta name="author" content="Sangeek Hyun">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                    Sangeek Hyun
                </p>
                <p>
                    <!-- I'm a research scientist at <a href="https://deepmind.google/">Google DeepMind</a> in San Francisco, where I lead a small team that mostly works on <a href="https://www.matthewtancik.com/nerf">NeRF</a>. -->
                    I am a Ph.D. candidate in the Visual Computing Lab (VCLab) at Sungkyunkwan University, supervised by Prof. Jae-Pil Heo.
                    I received my Master's and Bachelor's degrees from Sungkyunkwan University.
                    My research interests include various tasks in machine learning and computer vision, with a particular focus on generative models and video understanding.
                </p>

                <p style="text-align:center">
                  <a href="mailto:hse1032@gmail.com">Email</a> &nbsp;/&nbsp;
                  <!-- <a href="data/JonBarron-CV.pdf">CV</a> &nbsp;/&nbsp; -->
                  <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp;/&nbsp; -->
                  <a href="https://scholar.google.com/citations?user=xbaJDBwAAAAJ&hl">Google Scholar</a> &nbsp;/&nbsp;
                  <!-- <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp;/&nbsp; -->
                  <a href="https://github.com/hse1032/">Github</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/sangeek-hyun-33837a316/">LinkedIn</a>
                </p>

              </td>
              <td style="padding:2.5%;width:20%;max-width:20%">
                <a href="static/images/SangeekHyun2.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="static/images/SangeekHyun2.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                    Recently my interest has been in 3D generative models using Generative Adversarial Networks and gaussian splatting. I am also interested in various generation tasks using large-scale diffusion models.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>



            <!-- projects -->

            <tr onmouseout="nuvo_stop()" onmouseover="nuvo_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src='static/images/gsgan_cat.jpg' width=100%>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://hse1032.github.io/gsgan">
                    <span class="papertitle">Adversarial Generation of Hierarchical Gaussians for 3D Generative Model</span>
                  </a>
                  <br>
                  <strong>Sangeek Hyun</strong>, Jae-Pil Heo
                  <br>
                  <em>Arxiv</em>, 2024
                  <br>
                  <a href="https://hse1032.github.io/gsgan">project page</a>
                  /
                  <!-- <a href="https://www.youtube.com/watch?v=hmJiOSTDQZI">video</a>
                  / -->
                  <a href="https://github.com/hse1032/Adversarial-Generation-of-Hierarchical-Gaussians-for-3D-Generative-Model">code</a>
                  /
                  <a href="https://arxiv.org/abs/2406.02968">arXiv</a>
                  <p></p>
                  <p>
                  First 3D GANs utilize gaussian splatting without any structural priors, achieving faster rendering speed at high-resolution data compared to NeRFs.
                  </p>
                </td>
              </tr>

              
              <tr onmouseout="nuvo_stop()" onmouseover="nuvo_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src='static/images/styleid.jpg' width=100%>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://jiwoogit.github.io/StyleID_site/">
                    <span class="papertitle">Style Injection in Diffusion: A Training-free Approach for Adapting Large-scale Diffusion Models for Style Transfer</span>
                  </a>
                  <br>
                  Jiwoo Chung*, <strong>Sangeek Hyun*</strong>, SuBeen Lee, Jae-Pil Heo (*: Equal contribution)
                  <br>
                  <em>CVPR</em>, 2024 <strong style="color:red;">(Highlight)</strong>
                  <br>
                  <a href="https://jiwoogit.github.io/StyleID_site/">project page</a>
                  /
                  <a href="https://github.com/jiwoogit/StyleID">code</a>
                  /
                  <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Chung_Style_Injection_in_Diffusion_A_Training-free_Approach_for_Adapting_Large-scale_CVPR_2024_paper.html">paper</a>
                  /
                  <a href="https://arxiv.org/abs/2312.09008">arXiv</a>
                  <p></p>
                  <p>
                    Training-free style transfer utilizes large-scale diffusion models by manipulating the attention features.
                  </p>
                </td>
              </tr>

              <tr onmouseout="nuvo_stop()" onmouseover="nuvo_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src='static/images/dcpgan.jpg' width=100%>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://github.com/jiwoogit/DCP-GAN">
                    <span class="papertitle">Diversity-aware Channel Pruning for StyleGAN Compression</span>
                  </a>
                  <br>
                  Jiwoo Chung, <strong>Sangeek Hyun</strong>, Sang-Heon Shim, Jae-Pil Heo
                  <br>
                  <em>CVPR</em>, 2024
                  <br>
                  <!-- <a href="https://jiwoogit.github.io/StyleID_site/">project page</a>
                  / -->
                  <a href="https://github.com/jiwoogit/DCP-GAN">code</a>
                  /
                  <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Chung_Diversity-aware_Channel_Pruning_for_StyleGAN_Compression_CVPR_2024_paper.html">paper</a>
                  /
                  <a href="https://arxiv.org/abs/2403.13548">arXiv</a>
                  <p></p>
                  <p>
                    GAN compression technique by pruning the diversity-aware channels in StyleGAN architecture.
                  </p>
                </td>
              </tr>

              <tr onmouseout="nuvo_stop()" onmouseover="nuvo_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src='static/images/tbs.png' width=100%>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://github.com/suhopark0706/tbsnet">
                    <span class="papertitle">Task-disruptive Background Suppression for Few-Shot Segmentation</span>
                  </a>
                  <br>
                  Suho Park, SuBeen Lee, <strong>Sangeek Hyun</strong>, Hyun Seok Seong, Jae-Pil Heo
                  <br>
                  <em>AAAI</em>, 2024
                  <br>
                  <!-- <a href="https://jiwoogit.github.io/StyleID_site/">project page</a>
                  / -->
                  <a href="https://github.com/suhopark0706/tbsnet">code</a>
                  /
                  <a href="https://ojs.aaai.org/index.php/AAAI/article/view/28242/28479">paper</a>
                  /
                  <a href="https://arxiv.org/abs/2312.15894">arXiv</a>
                  <p></p>
                  <p>
                    Task-disruptive Background Suppression mitigates the negative impact of dissimilar or target-similar support backgrounds, improving the accuracy of segmenting novel target objects in query images.
                  </p>
                </td>
              </tr>

              <tr onmouseout="nuvo_stop()" onmouseover="nuvo_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src='static/images/cgdetr.png' width=100%>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://github.com/wjun0830/CGDETR">
                    <span class="papertitle">Correlation-guided Query-Dependency Calibration in Video Representation Learning for Temporal Grounding</span>
                  </a>
                  <br>
                  WonJun Moon, <strong>Sangeek Hyun</strong>, SuBeen Lee, Jae-Pil Heo
                  <br>
                  <em>Arxiv</em>, 2023
                  <br>
                  <a href="https://github.com/wjun0830/CGDETR">code</a>
                  /
                  <!-- <a href="https://www.youtube.com/watch?v=hmJiOSTDQZI">video</a>
                  / -->
                  <a href="https://arxiv.org/abs/2311.08835">arXiv</a>
                  <p></p>
                  <p>
                    CG-DETR improves temporal grounding by using adaptive cross-attention and clip-word correlation to accurately identify video highlights corresponding to textual descriptions.
                  </p>
                </td>
              </tr>

              <tr onmouseout="nuvo_stop()" onmouseover="nuvo_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src='static/images/freqvideogan.png' width=100%>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://github.com/hse1032/Frequency-based-Motion-Representation-for-Video-GANs">
                    <span class="papertitle">Frequency-based motion representation for video generative adversarial networks</span>
                  </a>
                  <br>
                  <strong>Sangeek Hyun</strong>, Jaihyun Lew, Jiwoo Chung, Euiyeon Kim, Jae-Pil Heo
                  <br>
                  <em>TIP</em>, 2023
                  <br>
                  <a href="https://github.com/hse1032/Frequency-based-Motion-Representation-for-Video-GANs">code</a>
                  /
                  <a href="https://ieeexplore.ieee.org/abstract/document/10183834/">paper</a>
                  <p></p>
                  <p>
                    Propose a frequency-based motion representation for video GANs, enabling speed-aware motion generation, which improves video quality and editing capability.
                  </p>
                </td>
              </tr>

              <tr onmouseout="nuvo_stop()" onmouseover="nuvo_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src='static/images/dunq.png' width=100%>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Noh_Disentangled_Representation_Learning_for_Unsupervised_Neural_Quantization_CVPR_2023_paper.html">
                    <span class="papertitle">Disentangled Representation Learning for Unsupervised Neural Quantization</span>
                  </a>
                  <br>
                  Haechan Noh, <strong>Sangeek Hyun</strong>, Woojin Jeong, Hanshin Lim, Jae-Pil Heo
                  <br>
                  <em>CVPR</em>, 2023
                  <br>
                  <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Noh_Disentangled_Representation_Learning_for_Unsupervised_Neural_Quantization_CVPR_2023_paper.html">paper</a>
                  <p></p>
                  <p>
                    Disentangled representation learning for unsupervised neural quantization addresses deep learning quantizers' limitations in leveraging residual vector space, enhancing search efficiency and quality.
                  </p>
                </td>
              </tr>

              <tr onmouseout="nuvo_stop()" onmouseover="nuvo_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src='static/images/qddetr.png' width=100%>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://wjun0830.github.io/cvpr2023/QDDETR/">
                    <span class="papertitle">Query-dependent video representation for moment retrieval and highlight detection</span>
                  </a>
                  <br>
                  WonJun Moon*, <strong>Sangeek Hyun*</strong>, SangUk Park, Dongchan Park, Jae-Pil Heo (*: Equal contribution)
                  <br>
                  <em>CVPR</em>, 2023
                  <br>
                  <a href="https://wjun0830.github.io/cvpr2023/QDDETR/">project page</a>
                  /
                  <a href="https://github.com/wjun0830/QD-DETR">code</a>
                  /
                  <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Moon_Query-Dependent_Video_Representation_for_Moment_Retrieval_and_Highlight_Detection_CVPR_2023_paper.html">paper</a>
                  /
                  <a href="https://arxiv.org/abs/2303.13874">arXiv</a>
                  <p></p>
                  <p>
                    Query-Dependent DETR improves video moment retrieval and highlight detection by enhancing query-video relevance and using negative pairs to refine saliency prediction.
                  </p>
                </td>
              </tr>

              <tr onmouseout="nuvo_stop()" onmouseover="nuvo_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src='static/images/lap.png' width=100%>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://openaccess.thecvf.com/content/CVPR2022/html/Shim_Local_Attention_Pyramid_for_Scene_Image_Generation_CVPR_2022_paper.html">
                    <span class="papertitle">Local attention pyramid for scene image generation</span>
                  </a>
                  <br>
                  Sang-Heon Shim, <strong>Sangeek Hyun</strong>, DaeHyun Bae, Jae-Pil Heo
                  <br>
                  <em>CVPR</em>, 2022
                  <br>
                  <a href="https://openaccess.thecvf.com/content/CVPR2022/html/Shim_Local_Attention_Pyramid_for_Scene_Image_Generation_CVPR_2022_paper.html">paper</a>
                  <p></p>
                  <p>
                    The Local Attention Pyramid (LAP) module addresses class-wise visual quality imbalance in GAN-generated scene images by enhancing attention to diverse object classes, particularly small and less frequent ones.
                  </p>
                </td>
              </tr>

              <tr onmouseout="nuvo_stop()" onmouseover="nuvo_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src='static/images/svgan.png' width=100%>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://openaccess.thecvf.com/content/CVPR2021/html/Hyun_Self-Supervised_Video_GANs_Learning_for_Appearance_Consistency_and_Motion_Coherency_CVPR_2021_paper.html">
                    <span class="papertitle">Self-Supervised Video GANs: Learning for Appearance Consistency and Motion Coherency</span>
                  </a>
                  <br>
                  <Strong>Sangeek Hyun</Strong>, Jihwan Kim, Jae-Pil Heo
                  <br>
                  <em>CVPR</em>, 2021
                  <br>
                  <a href="https://openaccess.thecvf.com/content/CVPR2021/html/Hyun_Self-Supervised_Video_GANs_Learning_for_Appearance_Consistency_and_Motion_Coherency_CVPR_2021_paper.html">paper</a>
                  <p></p>
                  <p>
                    Self-supervised approaches with dual discriminators improve video GANs by ensuring appearance consistency and motion coherency through contrastive learning and temporal structure puzzles.
                  </p>
                </td>
              </tr>

              <tr onmouseout="nuvo_stop()" onmouseover="nuvo_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src='static/images/varsr.png' width=100%>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://link.springer.com/chapter/10.1007/978-3-030-58592-1_26">
                    <span class="papertitle">VarSR: Variational Super-Resolution Network for Very Low Resolution Images</span>
                  </a>
                  <br>
                  <Strong>Sangeek Hyun</Strong>, Jae-Pil Heo
                  <br>
                  <em>ECCV</em>, 2020
                  <br>
                  <a href="https://link.springer.com/chapter/10.1007/978-3-030-58592-1_26">paper</a>
                  <p></p>
                  <p>
                    VarSR leverages latent distributions to address the many-to-one nature of single image super-resolution, generating diverse high-resolution images from low-resolution inputs.
                  </p>
                </td>
              </tr>

    </table>
  </body>
</html>